{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Multi Layer Perceptron\n",
    "Implement a Multi Layer Perceptron (MLP) using raw PyTorch Tensor operations. Avoid using APIs such as torch.nn or torch.functional. You are also\n",
    "not allowed to use neither torch.data nor torchvision.transforms. You can\n",
    "use torchvision.datasets.MNIST.\n",
    "The MLP architecture should consist of 784 input neurons, 100 hidden neurons, and 10 output neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run your model both on CPU and GPU and check the runtime. If you do\n",
    "not have a GPU available, use Google Colab and copy your script in a notebook\n",
    "cell (your script should be able to run on both CPU and GPU depending on a\n",
    "device parameter passed from the outside).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
    "        # (cuda:1).\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mos')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(x) -> Tensor:\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        if isinstance(x[0], Tensor):\n",
    "            return torch.stack(x)\n",
    "        return torch.tensor(x)\n",
    "    raise \"Not supported yet\"\n",
    "    # see torch\\utils\\data\\_utils\\collate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(x: Tensor) -> Tensor:\n",
    "    return torch.eye(x.max() + 1)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path: str = \"./data\", train: bool = True, pin_memory: bool = True):\n",
    "    mnist_raw = MNIST(path, download=True, train=train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in mnist_raw:\n",
    "        tensor = torch.from_numpy(np.array(image))\n",
    "        mnist_data.append(tensor)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    mnist_data = collate(mnist_data).float()  # shape 60000, 28, 28\n",
    "    mnist_data = mnist_data.flatten(start_dim=1)  # shape 60000, 784\n",
    "    mnist_data /= mnist_data.max()  # normalize the data\n",
    "    mnist_labels = collate(mnist_labels)  # shape 60000\n",
    "    if train:\n",
    "        mnist_labels = to_one_hot(mnist_labels)  # shape 60000, 10\n",
    "    if pin_memory:\n",
    "        return mnist_data.pin_memory(), mnist_labels.pin_memory()\n",
    "    return mnist_data, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 2 activation functions for best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: Tensor) -> Tensor: # for the last layer\n",
    "    return z.softmax(dim=1)\n",
    "\n",
    "def sigmoid(z: Tensor): # for the hidden layer\n",
    "    return z.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x: Tensor, y: Tensor, y1_hat: Tensor, y2_hat: Tensor, w2: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    # Compute the error in the final layer\n",
    "    error_layer2 = y2_hat - y # Shape: (batch_size, 10)\n",
    "\n",
    "    # Calculate the gradients for weights and biases in the final layer\n",
    "    delta_w2 = y1_hat.T @ error_layer2 # Shape: (100, 10)\n",
    "    delta_b2 = error_layer2.mean(dim=0) # Shape: (10,)\n",
    "\n",
    "    # Compute the error in the hidden layer\n",
    "    error_layer1 = y1_hat * (1 - y1_hat) * (w2 @ error_layer2.T).T # Shape: (batch_size, 100)\n",
    "\n",
    "    # Calculate the gradients for weights and biases in the hidden layer\n",
    "    delta_w1 = x.T @ error_layer1 # Shape: (784, 100)\n",
    "    delta_b1 = error_layer1.mean(dim=0) # Shape: (100,)\n",
    "    \n",
    "    return delta_w1, delta_b1, delta_w2, delta_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x: Tensor, y: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, lr: float) -> Tuple[Tensor, Tensor, Tensor, Tensor, float]:\n",
    "    # Forward Propagation\n",
    "    y1_hat = sigmoid(forward(x, w1, b1)) # output of the hidden layer\n",
    "    y2_hat = softmax(forward(y1_hat, w2, b2)) # output of the final layer\n",
    "\n",
    "    # calculating the loss\n",
    "    loss = torch.nn.functional.cross_entropy(y2_hat, y)\n",
    "\n",
    "    # Backward Propagation\n",
    "    delta_w1, delta_b1, delta_w2, delta_b2 = backward(x, y, y1_hat, y2_hat, w2)\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    w1 -= lr * delta_w1\n",
    "    b1 -= lr * delta_b1\n",
    "    w2 -= lr * delta_w2\n",
    "    b2 -= lr * delta_b2\n",
    "    return w1, b1, w2, b2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data: Tensor, labels: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, lr: float, batch_size: int) \\\n",
    "        -> Tuple[Tensor, Tensor, Tensor, Tensor, float]:\n",
    "    non_blocking = w1.device.type == 'cuda'\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        # Extracting a batch of data and labels\n",
    "        x = data[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        y = labels[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        w1, b1, w2, b2, batch_loss = train_batch(x, y, w1, b1, w2, b2, lr)\n",
    "        # Accumulate batch loss to compute epoch loss\n",
    "        epoch_loss += batch_loss \n",
    "\n",
    "        # Compute average epoch loss\n",
    "        avg_epoch_loss = epoch_loss / batch_size\n",
    "    return w1, b1, w2, b2, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data: Tensor, labels: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, batch_size: int) -> float:\n",
    "    # Labels are not one hot encoded, because we do not need them as one hot.\n",
    "    total_correct_predictions = 0\n",
    "    total_len = data.shape[0]\n",
    "    non_blocking = w1.device.type == 'cuda'\n",
    "    for i in range(0, total_len, batch_size):\n",
    "\n",
    "        # Extract a batch of data and labels\n",
    "        x = data[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        y = labels[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "\n",
    "        # Forward pass to get predicted distribution\n",
    "        predicted_distribution = softmax(forward(sigmoid(forward(x, w1, b1)), w2, b2)) \n",
    "\n",
    "        # check torch.max documentation\n",
    "        predicted_max_value, predicted_max_value_indices = torch.max(predicted_distribution, dim=1)\n",
    "        # we check if the indices of the max value per line correspond to the correct label. We get a boolean mask\n",
    "        # with True where the indices are the same, false otherwise\n",
    "        equality_mask = predicted_max_value_indices == y\n",
    "        # We sum the boolean mask, and get the number of True values in the mask. We use .item() to get the value out of\n",
    "        # the tensor\n",
    "        correct_predictions = equality_mask.sum().item()\n",
    "        # correct_predictions = (torch.max(predicted_distribution, dim=1)[1] == y).sum().item()\n",
    "        total_correct_predictions += correct_predictions\n",
    "\n",
    "    return total_correct_predictions / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int = 1000, device: torch.device = get_default_device()):\n",
    "    print(f\"Using device {device}\")\n",
    "    pin_memory = device.type == 'cuda'  # Check the provided references.\n",
    "    # Initialize w and b\n",
    "    w1 = torch.normal(0, 1 / np.sqrt(784) , (784, 100), device=device) # Xavier/Glorot initialization\n",
    "    b1 = torch.zeros((1, 100), device=device)\n",
    "    w2 = torch.normal(0, 1 / np.sqrt(784), (100, 10), device=device)\n",
    "    b2 = torch.zeros((1, 10), device=device)\n",
    "    lr = 0.001\n",
    "    batch_size = 100\n",
    "    eval_batch_size = 500\n",
    "    data, labels = load_mnist(train=True, pin_memory=pin_memory)\n",
    "    data_test, labels_test = load_mnist(train=False, pin_memory=pin_memory)\n",
    "    # Set up progress bar for epochs\n",
    "    epochs = tqdm(range(epochs))\n",
    "    total_loss = 0\n",
    "    for epoch in epochs:\n",
    "        epoch_loss = 0\n",
    "        w1, b1, w2, b2, epoch_loss = train_epoch(data, labels, w1, b1, w2, b2, lr, batch_size)\n",
    "        total_loss += epoch_loss\n",
    "        accuracy = evaluate(data_test, labels_test, w1, b1, w2, b2, eval_batch_size)\n",
    "        # Update progress bar description\n",
    "        epochs.set_postfix_str(f\"accuracy = {accuracy}, epoch_loss = {epoch_loss}, total_loss = {total_loss}\")\n",
    "        if epoch % 100 == 0:\n",
    "            lr *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(300) # for GPU\n",
    "train(300, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the code in one block to test in google collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
    "        # (cuda:1).\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mos')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def collate(x) -> Tensor:\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        if isinstance(x[0], Tensor):\n",
    "            return torch.stack(x)\n",
    "        return torch.tensor(x)\n",
    "    raise \"Not supported yet\"\n",
    "    # see torch\\utils\\data\\_utils\\collate.py\n",
    "\n",
    "def to_one_hot(x: Tensor) -> Tensor:\n",
    "    return torch.eye(x.max() + 1)[x]\n",
    "\n",
    "\n",
    "def load_mnist(path: str = \"./data\", train: bool = True, pin_memory: bool = True):\n",
    "    mnist_raw = MNIST(path, download=True, train=train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in mnist_raw:\n",
    "        tensor = torch.from_numpy(np.array(image))\n",
    "        mnist_data.append(tensor)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    mnist_data = collate(mnist_data).float()  # shape 60000, 28, 28\n",
    "    mnist_data = mnist_data.flatten(start_dim=1)  # shape 60000, 784\n",
    "    mnist_data /= mnist_data.max()  # normalize the data\n",
    "    mnist_labels = collate(mnist_labels)  # shape 60000\n",
    "    if train:\n",
    "        mnist_labels = to_one_hot(mnist_labels)  # shape 60000, 10\n",
    "    if pin_memory:\n",
    "        return mnist_data.pin_memory(), mnist_labels.pin_memory()\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "def forward(x: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
    "    return x @ w + b\n",
    "\n",
    "def softmax(z: Tensor) -> Tensor: # for the last layer\n",
    "    return z.softmax(dim=1)\n",
    "\n",
    "def sigmoid(z: Tensor): # for the hidden layer\n",
    "    return z.sigmoid()\n",
    "\n",
    "def backward(x: Tensor, y: Tensor, y1_hat: Tensor, y2_hat: Tensor, w2: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    # Compute the error in the final layer\n",
    "    error_layer2 = y2_hat - y # Shape: (batch_size, 10)\n",
    "\n",
    "    # Calculate the gradients for weights and biases in the final layer\n",
    "    delta_w2 = y1_hat.T @ error_layer2 # Shape: (100, 10)\n",
    "    delta_b2 = error_layer2.mean(dim=0) # Shape: (10,)\n",
    "\n",
    "    # Compute the error in the hidden layer\n",
    "    error_layer1 = y1_hat * (1 - y1_hat) * (w2 @ error_layer2.T).T # Shape: (batch_size, 100)\n",
    "\n",
    "    # Calculate the gradients for weights and biases in the hidden layer\n",
    "    delta_w1 = x.T @ error_layer1 # Shape: (784, 100)\n",
    "    delta_b1 = error_layer1.mean(dim=0) # Shape: (100,)\n",
    "    \n",
    "    return delta_w1, delta_b1, delta_w2, delta_b2\n",
    "\n",
    "\n",
    "def train_batch(x: Tensor, y: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, lr: float) -> Tuple[Tensor, Tensor, Tensor, Tensor, float]:\n",
    "    # Forward Propagation\n",
    "    y1_hat = sigmoid(forward(x, w1, b1)) # output of the hidden layer\n",
    "    y2_hat = softmax(forward(y1_hat, w2, b2)) # output of the final layer\n",
    "\n",
    "    # calculating the loss\n",
    "    loss = torch.nn.functional.cross_entropy(y2_hat, y)\n",
    "\n",
    "    # Backward Propagation\n",
    "    delta_w1, delta_b1, delta_w2, delta_b2 = backward(x, y, y1_hat, y2_hat, w2)\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    w1 -= lr * delta_w1\n",
    "    b1 -= lr * delta_b1\n",
    "    w2 -= lr * delta_w2\n",
    "    b2 -= lr * delta_b2\n",
    "    return w1, b1, w2, b2, loss\n",
    "\n",
    "def train_epoch(data: Tensor, labels: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, lr: float, batch_size: int) \\\n",
    "        -> Tuple[Tensor, Tensor, Tensor, Tensor, float]:\n",
    "    non_blocking = w1.device.type == 'cuda'\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        # Extracting a batch of data and labels\n",
    "        x = data[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        y = labels[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        w1, b1, w2, b2, batch_loss = train_batch(x, y, w1, b1, w2, b2, lr)\n",
    "        # Accumulate batch loss to compute epoch loss\n",
    "        epoch_loss += batch_loss \n",
    "\n",
    "        # Compute average epoch loss\n",
    "        avg_epoch_loss = epoch_loss / batch_size\n",
    "    return w1, b1, w2, b2, avg_epoch_loss\n",
    "\n",
    "def evaluate(data: Tensor, labels: Tensor, w1: Tensor, b1: Tensor, w2: Tensor, b2: Tensor, batch_size: int) -> float:\n",
    "    # Labels are not one hot encoded, because we do not need them as one hot.\n",
    "    total_correct_predictions = 0\n",
    "    total_len = data.shape[0]\n",
    "    non_blocking = w1.device.type == 'cuda'\n",
    "    for i in range(0, total_len, batch_size):\n",
    "\n",
    "        # Extract a batch of data and labels\n",
    "        x = data[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "        y = labels[i: i + batch_size].to(w1.device, non_blocking=non_blocking)\n",
    "\n",
    "        # Forward pass to get predicted distribution\n",
    "        predicted_distribution = softmax(forward(sigmoid(forward(x, w1, b1)), w2, b2)) \n",
    "\n",
    "        # check torch.max documentation\n",
    "        predicted_max_value, predicted_max_value_indices = torch.max(predicted_distribution, dim=1)\n",
    "        # we check if the indices of the max value per line correspond to the correct label. We get a boolean mask\n",
    "        # with True where the indices are the same, false otherwise\n",
    "        equality_mask = predicted_max_value_indices == y\n",
    "        # We sum the boolean mask, and get the number of True values in the mask. We use .item() to get the value out of\n",
    "        # the tensor\n",
    "        correct_predictions = equality_mask.sum().item()\n",
    "        # correct_predictions = (torch.max(predicted_distribution, dim=1)[1] == y).sum().item()\n",
    "        total_correct_predictions += correct_predictions\n",
    "\n",
    "    return total_correct_predictions / total_len\n",
    "\n",
    "def train(epochs: int = 1000, device: torch.device = get_default_device()):\n",
    "    print(f\"Using device {device}\")\n",
    "    pin_memory = device.type == 'cuda'  # Check the provided references.\n",
    "    # Initialize w and b\n",
    "    w1 = torch.normal(0, 1 / np.sqrt(784) , (784, 100), device=device) # Xavier/Glorot initialization\n",
    "    b1 = torch.zeros((1, 100), device=device)\n",
    "    w2 = torch.normal(0, 1 / np.sqrt(784), (100, 10), device=device)\n",
    "    b2 = torch.zeros((1, 10), device=device)\n",
    "    lr = 0.001\n",
    "    batch_size = 100\n",
    "    eval_batch_size = 500\n",
    "    data, labels = load_mnist(train=True, pin_memory=pin_memory)\n",
    "    data_test, labels_test = load_mnist(train=False, pin_memory=pin_memory)\n",
    "    # Set up progress bar for epochs\n",
    "    epochs = tqdm(range(epochs))\n",
    "    total_loss = 0\n",
    "    for epoch in epochs:\n",
    "        epoch_loss = 0\n",
    "        w1, b1, w2, b2, epoch_loss = train_epoch(data, labels, w1, b1, w2, b2, lr, batch_size)\n",
    "        total_loss += epoch_loss\n",
    "        accuracy = evaluate(data_test, labels_test, w1, b1, w2, b2, eval_batch_size)\n",
    "        # Update progress bar description\n",
    "        epochs.set_postfix_str(f\"accuracy = {accuracy}, epoch_loss = {epoch_loss}, total_loss = {total_loss}\")\n",
    "        if epoch % 100 == 0:\n",
    "            lr *= 0.90\n",
    "\n",
    "train(300) # for GPU\n",
    "train(300, torch.device('cpu'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
