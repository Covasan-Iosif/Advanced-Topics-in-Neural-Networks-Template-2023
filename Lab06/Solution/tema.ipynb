{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Covasan-Iosif/Advanced-Topics-in-Neural-Networks-Template-2023/blob/main/Lab06/Solution/tema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - CNN\n",
    "## Homework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previously built pipeline (or implement a new one), implement a CNN and test it on the\n",
    "CIFAR-10 dataset.\n",
    "Implement the Conv2d layer by hand, using tensor operations. Your implementation needs to\n",
    "be valid. Test that on the same input and weights, your implementation, and nn.Conv2d, have the\n",
    "same output (use the example code below) (4 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Handmade_conv2d_implementation(torch.nn.Module): # (input, weight, bias=None, stride=1, padding=0)\n",
    "    def __init__(self, weights):\n",
    "        super(Handmade_conv2d_implementation, self).__init__()\n",
    "        self.weights = weights # Conv kernel\n",
    "\n",
    "    def forward(self, x): # Forward Pass\n",
    "        batch_size, input_channels, input_height, input_width = x.size()\n",
    "        output_channels, input_channels, kernel_height, kernel_width = self.weights.size()\n",
    "\n",
    "        # Initialize the output tensor (2DConvolution with stride=1 and 0 padding)\n",
    "        output_height = input_height - kernel_height + 1\n",
    "        output_width = input_width - kernel_width + 1\n",
    "        output = torch.zeros(batch_size, output_channels, output_height, output_width) # (1, 2, 7, 8)\n",
    "\n",
    "        # Perform the convolution\n",
    "        # Iterate through dimensions\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(output_channels):\n",
    "                for h_out in range(output_height):\n",
    "                    for w_out in range(output_width):\n",
    "                        # Extract the current patch from the input tensor x \n",
    "                        patch = x[b, :, h_out : h_out + kernel_height, w_out : w_out + kernel_width]\n",
    "                        # Patch is a sub-tensor of dimensions (in_channels, kernel_height, kernel_width) \n",
    "                        # located at the current coordinates (h_out, w_out) in the output tensor.\n",
    "\n",
    "                        # Perform element-wise multiplication between the patch and the weights \n",
    "                        # corresponding to the output channel\n",
    "                        output[b, c_out, h_out, w_out] = (patch * self.weights[c_out]).sum()\n",
    "                        # Sum the elements to get the convolution at a specific position in the output tensor\n",
    "\n",
    "        return output\n",
    "\n",
    "# Testing the custom Conv2D layer\n",
    "inp = torch.randn(1, 3, 10, 12)  # Input image (batch_size, input_channels, input_height, input_width) \n",
    "w = torch.randn(2, 3, 4, 5)  # Conv weights (output_channels, input_channels, kernel_height, kernel_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the handmade Conv2D layer\n",
    "custom_conv2d_layer = Handmade_conv2d_implementation(weights=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the handmade Conv2D layer\n",
    "out_custom = custom_conv2d_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "print(out_custom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the PyTorch built-in Conv2D layer\n",
    "out_builtin = torch.nn.functional.conv2d(inp, w) # stride=1 and 0 padding by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "print(out_builtin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6294e-06)\n"
     ]
    }
   ],
   "source": [
    "print((out_builtin - out_custom).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cele două layere produc aproximativ același rezultat\n"
     ]
    }
   ],
   "source": [
    "tolerance = 1e-5\n",
    "\n",
    "if torch.allclose(out_custom, out_builtin, atol=tolerance):\n",
    "    print(\"Cele două layere produc aproximativ același rezultat\")\n",
    "else:\n",
    "    print(\"Cele două layere nu produc același rezultat.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atnn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
